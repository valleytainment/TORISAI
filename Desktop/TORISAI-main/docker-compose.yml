"""
TORIS AI - Docker Compose Configuration
Implements containerized deployment with proper isolation
"""
version: "3.9"

services:
  # Ollama service for local LLM inference
  ollama:
    image: ollama/ollama:0.1.27
    container_name: torisai-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # TORIS AI backend API
  torisai-backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: torisai-backend
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - TORIS_API_TOKEN=${TORIS_API_TOKEN:-local-development-token}
    ports:
      - "8000:8000"
    volumes:
      - ./memory:/app/memory
      - ./chroma_db:/app/chroma_db
      - ./logs:/app/logs
    restart: unless-stopped
    command: "uvicorn torisai.api.main:app --host 0.0.0.0 --port 8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # TORIS AI frontend UI
  torisai-ui:
    build:
      context: ./ui
      dockerfile: Dockerfile.ui
    container_name: torisai-ui
    depends_on:
      torisai-backend:
        condition: service_healthy
    ports:
      - "7860:7860"
    environment:
      - BACKEND_URL=http://torisai-backend:8000
      - TORIS_API_TOKEN=${TORIS_API_TOKEN:-local-development-token}
    restart: unless-stopped

volumes:
  ollama_data:
    name: torisai-ollama-data
